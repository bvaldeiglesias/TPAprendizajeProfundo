{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "central-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-slovenia",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "laughing-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Bateria Completa 5 Cuerpos Excelente</td>\n",
       "      <td>DRUMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Cuaderno Anotador Espiral  Ben 10 3d Original ...</td>\n",
       "      <td>NOTEBOOKS_AND_WRITING_PADS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Fifa18 Ps4 Disco Fisico</td>\n",
       "      <td>VIDEO_GAMES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Botines Futbol adidas Messi 15.4 Cesped Hombre</td>\n",
       "      <td>FOOTBALL_SHOES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Chops Sublimados - Nagual</td>\n",
       "      <td>DRINKING_GLASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119095</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Plantas Medicinales De Valeriana Roja!!!!</td>\n",
       "      <td>PLANTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119096</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Patas Altas Sommier 25 Cm. - X 4 Unidades</td>\n",
       "      <td>BOX_SPRING_AND_MATTRESS_SETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119097</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Combo Funda Asiento Univ Cuero Automotor Cub A...</td>\n",
       "      <td>CAR_SEAT_COVERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119098</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Griferia Baño Fv Margot Lever Lavatorio Pared ...</td>\n",
       "      <td>BATHROOM_FAUCET_SETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119099</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Teléfono Inalámbrico Motorola Gate 4800-2 Duo ...</td>\n",
       "      <td>TELEPHONES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6119100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        language label_quality  \\\n",
       "0        spanish      reliable   \n",
       "1        spanish      reliable   \n",
       "2        spanish      reliable   \n",
       "3        spanish      reliable   \n",
       "4        spanish      reliable   \n",
       "...          ...           ...   \n",
       "6119095  spanish    unreliable   \n",
       "6119096  spanish    unreliable   \n",
       "6119097  spanish    unreliable   \n",
       "6119098  spanish    unreliable   \n",
       "6119099  spanish    unreliable   \n",
       "\n",
       "                                                     title  \\\n",
       "0                    Bateria Completa 5 Cuerpos Excelente    \n",
       "1        Cuaderno Anotador Espiral  Ben 10 3d Original ...   \n",
       "2                                  Fifa18 Ps4 Disco Fisico   \n",
       "3           Botines Futbol adidas Messi 15.4 Cesped Hombre   \n",
       "4                                Chops Sublimados - Nagual   \n",
       "...                                                    ...   \n",
       "6119095          Plantas Medicinales De Valeriana Roja!!!!   \n",
       "6119096          Patas Altas Sommier 25 Cm. - X 4 Unidades   \n",
       "6119097  Combo Funda Asiento Univ Cuero Automotor Cub A...   \n",
       "6119098  Griferia Baño Fv Margot Lever Lavatorio Pared ...   \n",
       "6119099  Teléfono Inalámbrico Motorola Gate 4800-2 Duo ...   \n",
       "\n",
       "                             category  \n",
       "0                               DRUMS  \n",
       "1          NOTEBOOKS_AND_WRITING_PADS  \n",
       "2                         VIDEO_GAMES  \n",
       "3                      FOOTBALL_SHOES  \n",
       "4                    DRINKING_GLASSES  \n",
       "...                               ...  \n",
       "6119095                        PLANTS  \n",
       "6119096  BOX_SPRING_AND_MATTRESS_SETS  \n",
       "6119097               CAR_SEAT_COVERS  \n",
       "6119098          BATHROOM_FAUCET_SETS  \n",
       "6119099                    TELEPHONES  \n",
       "\n",
       "[6119100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./spanish.train.csv.gz\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6119100 entries, 0 to 6119099\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   language       object\n",
      " 1   label_quality  object\n",
      " 2   title          object\n",
      " 3   category       object\n",
      "dtypes: object(4)\n",
      "memory usage: 186.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "related-houston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6119100</td>\n",
       "      <td>6119100</td>\n",
       "      <td>6119100</td>\n",
       "      <td>6119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6119100</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>40 Dijes Corazones Colores Surtidos</td>\n",
       "      <td>BOOKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6119100</td>\n",
       "      <td>5635232</td>\n",
       "      <td>1</td>\n",
       "      <td>19010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       language label_quality                                 title category\n",
       "count   6119100       6119100                               6119100  6119100\n",
       "unique        1             2                               6119100      632\n",
       "top     spanish    unreliable  40 Dijes Corazones Colores Surtidos     BOOKS\n",
       "freq    6119100       5635232                                     1    19010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-residence",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "parliamentary-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"title\"],\n",
    "            \"target\": self.dataset.loc[item, \"category\"]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-moses",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decent-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 ignore_header=True, \n",
    "                 filters=None, \n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "#                 preprocessing.strip_tags,\n",
    "#                 preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "#                 preprocessing.strip_numeric,\n",
    "#                 preprocessing.remove_stopwords,\n",
    "#                 preprocessing.strip_short,\n",
    "            ]\n",
    "        \n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"title\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary and compactify it (make the indices continous)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0,\n",
    "            \"[UNK]\": 1\n",
    "        })\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-given",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pregnant-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded with 6119100 training elements and 63680 test elements\n",
      "Sample train element:\n",
      "{'data': [50001, 2, 50000, 3, 4], 'target': 196}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"./spanish.train.csv.gz\")\n",
    "test_dataset = pd.read_csv(\"./spanish.test.csv.gz\")\n",
    "\n",
    "preprocess = RawDataProcessor(dataset)\n",
    "\n",
    "# train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MeLiDataset(train_dataset, transform=preprocess)\n",
    "\n",
    "test_dataset = MeLiDataset(test_dataset, transform=preprocess)\n",
    "\n",
    "print(f\"Datasets loaded with {len(train_dataset)} training elements and {len(test_dataset)} test elements\")\n",
    "print(f\"Sample train element:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "present-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-outreach",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thousand-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                          collate_fn=pad_sequences, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n",
    "                         collate_fn=pad_sequences, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-treat",
   "metadata": {},
   "source": [
    "# NN Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiFasttextClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 word_embeddings, \n",
    "                 config,\n",
    "                 vocab_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
    "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.fc1 = nn.Linear(self.config.embed_size, self.config.hidden_size)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc2 = nn.Linear(self.config.hidden_size, self.config.output_size)\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_sent = self.embeddings(x).permute(1,0,2)\n",
    "        h = self.fc1(embedded_sent.mean(1))\n",
    "        z = self.fc2(h)\n",
    "        return self.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "restricted-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiMLPClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlimited-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiCNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "third-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensions=128,\n",
    "                 num_layers=1, dropout=0.5, bias=True,\n",
    "                 bidirectional=True):\n",
    "        \n",
    "        super(MeLiRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 300)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 1)\n",
    "        \n",
    "        # Instanciate the layers\n",
    "        self.encoder = nn.LSTM(**self.lstm_config)\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('linear', nn.Linear(**self.linear_config))\n",
    "        self.decoder.add_module('softmax',nn.LogSoftmax(dim=-1))\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-latin",
   "metadata": {},
   "source": [
    "# Training & ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "focal-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'AprendizajeProfundo_MLP_experiment' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ecb79cfaa4c07af7f1252885b8039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9395372384e242308f1227776fc35e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b47c7dd2b4a4ba1b151d36ba359f27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5801115b63034a1aaafd2e6d88f76fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41efb8cc1d024e32858edcdf554164d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ffff4d3bbc4e3e97d1bb9e93a9520b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a96f71ab984b2abb106e888b204f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463c1e84b58643ee981b25c7a1ac5f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c223a0c32f4b62a6e138729b1d3063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda336a19fc9468197ba9e403006bb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03afca537653497a902f07c60a109b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd577976bd19469380aa0b966935f353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"AprendizajeProfundo_MLP_experiment\")\n",
    "\n",
    "n_epochs = 5\n",
    "embedding_size = 50\n",
    "freeze_embedding = True\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_name\", \"MeLiMLPClassifier\")\n",
    "    mlflow.log_param(\"freeze_embedding\", freeze_embedding)\n",
    "    mlflow.log_params({\n",
    "        \"embedding_size\": embedding_size,\n",
    "        \"hidden1_size\": 128,\n",
    "        \"hidden2_size\": 128\n",
    "    })\n",
    "    model = MeLiMLPClassifier(\"./glove.6B.50d.txt.gz\", preprocess.dictionary, embedding_size, freeze_embedding)\n",
    "    loss = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    for epoch in trange(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch[\"data\"])\n",
    "            loss_value = loss(output, batch[\"target\"].view(-1, 1))\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())        \n",
    "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch[\"data\"])\n",
    "            running_loss.append(\n",
    "                loss(output, batch[\"target\"].view(-1, 1)).item()\n",
    "            )\n",
    "            targets.extend(batch[\"target\"].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch[\"data\"])\n",
    "            targets.extend(batch[\"target\"].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "            f\"{tmpdirname}/predictions.csv.gz\", index=False\n",
    "        )\n",
    "        mlflow.log_artifact(f\"{tmpdirname}/predictions.csv.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
