{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-choice",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./spanish.train.csv.gz\")\n",
    "test_dataset = pd.read_csv(\"./spanish.test.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-offense",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "talented-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"title\"],\n",
    "            \"target\": self.dataset.loc[item, \"category\"]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-taylor",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binary-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 ignore_header=True, \n",
    "                 filters=None, \n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        \n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"title\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary and compactify it (make the indices continous)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0,\n",
    "            \"[UNK]\": 1\n",
    "        })\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = RawDataProcessor(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded with 6119100 training elements and 63680 test elements\n",
      "Sample train element:\n",
      "{'data': [50000, 50001, 2, 3], 'target': 196}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MeLiDataset(train_dataset, transform=preprocess)\n",
    "\n",
    "test_dataset = MeLiDataset(test_dataset, transform=preprocess)\n",
    "\n",
    "print(f\"Datasets loaded with {len(train_dataset)} training elements and {len(test_dataset)} test elements\")\n",
    "print(f\"Sample train element:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "important-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-chain",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "infectious-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS_COUNT = 100\n",
    "FILTERS_LENGTH = [2, 3, 4]\n",
    "\n",
    "pad_sequences = PadSequences(min_length=max(FILTERS_LENGTH))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True,\n",
    "                          collate_fn=pad_sequences, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False,\n",
    "                         collate_fn=pad_sequences, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-appendix",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominant-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiCNNClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings,\n",
    "                 output_layer=632):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.convs = []\n",
    "        for filter_lenght in FILTERS_LENGTH:\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(vector_size, FILTERS_COUNT, filter_lenght)\n",
    "            )\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.fc = nn.Linear(FILTERS_COUNT * len(FILTERS_LENGTH), 128)\n",
    "        self.output = nn.Linear(128, output_layer)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_global_max_pool(x, conv):\n",
    "        return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x).transpose(1, 2)  # Conv1d takes (batch, channel, seq_len)\n",
    "        x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-arbor",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strategic-drawing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1af5f510c34ad097f3487b330a281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d677c4ae6e88421a826fa114dad50395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68bd9ea1ded49fd967e29ff64312643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1733ab90a46c4220b74b2f15e254defe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49064e2833b4edaa35abf30497db20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b330b272c2a4c1aa5d03f4906ec65c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17127a7842f48af945f3180daa5c3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea7e11b1ab14cfd9f80c25b332f672a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d565fdf5b84c1eb45646dd0bf9e154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01117dcdff94474cadc69e130dc7af68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a04c4377655438e9c0af23b65cb83f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cd8ccc816c4347bcf42a6452c89ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d432293d3d4d41f88f8d62b9277a9a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d33a6a797b4411930fd8d7899231cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2118d962f77d4ee4b93d6ab821d47b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7123b284d74f0d8fd16ab8b2b4ca1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c552e9c9e147489c11dc77ca2ba1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a209e6527b427189f28bfdc36efe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af506e1b926e4333972d8816010b1bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ec2e296b81445aa3d97ddd623b5f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9dd847e654c8a8d7c4cb7d1fc2b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f3d2a17c2447848c125991563a6988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"AprendizajeProfundo_CNN_experiment\")\n",
    "\n",
    "EPOCHS = 10\n",
    "fc_size = 512\n",
    "vector_size = 50\n",
    "freeze_embedding = True\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_name\", \"MeLiCNNClassifier\")\n",
    "    mlflow.log_param(\"freeze_embedding\", freeze_embedding)\n",
    "    mlflow.log_params({\n",
    "        \"filters_count\": FILTERS_COUNT,\n",
    "        \"filters_length\": FILTERS_LENGTH,\n",
    "        \"fc_size\": fc_size,\n",
    "        \"vector_size\": vector_size\n",
    "    })\n",
    "    \n",
    "    model = MeLiCNNClassifier(\"./glove.6B.50d.txt.gz\",\n",
    "                              preprocess.dictionary,\n",
    "                              vector_size, freeze_embedding).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    for epoch in trange(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[\"data\"].long().to(device)\n",
    "            y = batch[\"target\"].long().to(device)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss_value = F.cross_entropy(y_pred, y)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())    \n",
    "            \n",
    "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            x = batch[\"data\"].long().to(device)\n",
    "            y = batch[\"target\"].long().to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            pred = torch.max(y_hat, 1)[1]\n",
    "            \n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "            targets.extend(batch[\"target\"])\n",
    "            predictions.extend(pred.cpu())\n",
    "        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
    " \n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch[\"data\"].long().to(device))\n",
    "            targets.extend(batch[\"target\"].cpu().numpy())\n",
    "            predictions.extend(output.cpu().max(1).indices.squeeze().detach().numpy())\n",
    "        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "            f\"{tmpdirname}/predictions.csv\", index=False\n",
    "        )\n",
    "        mlflow.log_artifact(f\"{tmpdirname}/predictions.csv\")\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
